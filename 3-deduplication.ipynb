{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2abc7877",
   "metadata": {},
   "source": [
    "# 5 - Ground Truth Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9883cde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS, KEYS, MODELS\n",
    "from serpapi import GoogleSearch\n",
    "import json\n",
    "from collections import Counter\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load API key\n",
    "keys = json.load(open(\"../apikeys.json\"))\n",
    "SERP_API_KEY = keys[\"SerpApi\"][\"key\"]\n",
    "\n",
    "# Load model (all-MiniLM-L6-v2 is fast and accurate)\n",
    "# model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "model = SentenceTransformer('../models/BERT-all-mpnet-base-v2')\n",
    "\n",
    "# SAVE MODEL (for offline use)\n",
    "# model.save('../models/BERT-all-mpnet-base-v2')\n",
    "# THEN TO LOAD IT AGAIN:\n",
    "# model = SentenceTransformer(modelPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b286c5",
   "metadata": {},
   "source": [
    "## Loading Results & Getting Rid of Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "334ed272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Proverbs: 2453\n",
      "Total Repetitions: 2547\n",
      "Deduped Unique Proverbs: 2453\n",
      "Number of duplicated proverbs: 141\n",
      "Average number of repetitions proverb: 19.06\n",
      "\n",
      "Most Frequent Repetitions:\n",
      "[714x] Comparison is the thief of joy.\n",
      "[617x] Don't believe everything you read on the internet just because there's a picture with a quote next to it.\n",
      "[340x] Don't compare your behind-the-scenes to everyone else's highlight reel.\n",
      "[209x] Don't believe everything you read on the internet.\n",
      "[78x] Don't compare your behind-the-scenes to someone else's highlight reel.\n",
      "[32x] In the world of the internet, you can be anything you want - it's up to you to decide what that is.\n",
      "[28x] Don't compare your behind-the-scenes footage with everyone else's highlight reel.\n",
      "[22x] Comparison is the thief of joy in the digital age.\n",
      "[21x] comparison is the thief of joy\n",
      "[20x] Comparison is the thief of joy; don't compare your behind-the-scenes to everyone else's highlight reel.\n"
     ]
    }
   ],
   "source": [
    "# Load file with texts & extract just the text field\n",
    "with open('outputs/di-hermes-5000-1.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "texts = [entry['text'] for entry in data]\n",
    "\n",
    "# With the texts loaded, the first step is to check \n",
    "# for exact duplicates and to reduce the number of texts\n",
    "# before using more computationally expensive similarity checks.\n",
    "\n",
    "# COUNT OCCURRENCES\n",
    "counts = Counter(texts)\n",
    "\n",
    "# FIND PROVERBS THAT APPEARED MORE THAN ONCE\n",
    "duplicates = {text: count for text, count in counts.items() if count > 1}\n",
    "print(f\"Unique Proverbs: {len(counts)}\")\n",
    "print(f\"Total Repetitions: {sum(duplicates.values()) - len(duplicates)}\")\n",
    "\n",
    "# REMOVE EXACT DUPLICATES & SORT BY LENGTH \n",
    "# sorting = shortest version is \"anchor\" for consolidation\n",
    "deduped = list(set(texts)) # If first method is used\n",
    "uniques = sorted(deduped, key=len)\n",
    "print(f\"Deduped Unique Proverbs: {len(uniques)}\")\n",
    "print(f\"Number of duplicated proverbs: {len(duplicates)}\")\n",
    "print(f\"Average number of repetitions proverb: {sum(duplicates.values()) / len(duplicates):.2f}\")\n",
    "\n",
    "# TO SEE REPETITIONS\n",
    "print(\"\\nMost Frequent Repetitions:\")\n",
    "for text, count in sorted(duplicates.items(), key=lambda x: x[1], reverse=True)[0:10]:\n",
    "    print(f\"[{count}x] {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57a7a986",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Compare current sentence against our accepted unique list\u001b[39;00m\n\u001b[32m     15\u001b[39m current_emb = model.encode(current, convert_to_tensor=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m unique_embs = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43munique_proverbs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m scores = util.cos_sim(current_emb, unique_embs)[\u001b[32m0\u001b[39m]\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# If it's not similar to anything we already have, add it\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/312/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/312/lib/python3.12/site-packages/sentence_transformers/SentenceTransformer.py:1090\u001b[39m, in \u001b[36mSentenceTransformer.encode\u001b[39m\u001b[34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, truncate_dim, pool, chunk_size, **kwargs)\u001b[39m\n\u001b[32m   1081\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mtoken_type_ids\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m features:\n\u001b[32m   1082\u001b[39m             features[\u001b[33m\"\u001b[39m\u001b[33mtoken_type_ids\u001b[39m\u001b[33m\"\u001b[39m] = torch.cat(\n\u001b[32m   1083\u001b[39m                 (\n\u001b[32m   1084\u001b[39m                     features[\u001b[33m\"\u001b[39m\u001b[33mtoken_type_ids\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m   1087\u001b[39m                 -\u001b[32m1\u001b[39m,\n\u001b[32m   1088\u001b[39m             )\n\u001b[32m-> \u001b[39m\u001b[32m1090\u001b[39m features = \u001b[43mbatch_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1091\u001b[39m features.update(extra_features)\n\u001b[32m   1093\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/312/lib/python3.12/site-packages/sentence_transformers/util/tensor.py:184\u001b[39m, in \u001b[36mbatch_to_device\u001b[39m\u001b[34m(batch, target_device)\u001b[39m\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[32m    183\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(batch[key], Tensor):\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m         batch[key] = \u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_device\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m batch\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# NOTA BENE: this takes anywhere from 8 to 30 minutes to run\n",
    "\n",
    "# EMPTY LIST TO HOLD LOOP OUTPUT\n",
    "unique_proverbs = []\n",
    "\n",
    "# THRESHOLD FOR SIMILARITY (lower = more aggressive)\n",
    "threshold = 0.50  # Aggressive grouping\n",
    "\n",
    "for current in uniques:\n",
    "    if not unique_proverbs:\n",
    "        unique_proverbs.append(current)\n",
    "        continue\n",
    "    \n",
    "    # Compare current sentence against our accepted unique list\n",
    "    current_emb = model.encode(current, convert_to_tensor=True)\n",
    "    unique_embs = model.encode(unique_proverbs, convert_to_tensor=True)\n",
    "    \n",
    "    scores = util.cos_sim(current_emb, unique_embs)[0]\n",
    "    \n",
    "    # If it's not similar to anything we already have, add it\n",
    "    if max(scores) < threshold:\n",
    "        unique_proverbs.append(current)\n",
    "\n",
    "# Display results\n",
    "print(f\"--- Original Count: {len(uniques)} | Final Count: {len(unique_proverbs)} ---\")\n",
    "for p in unique_proverbs:\n",
    "    print(f\"‚úì {p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c26991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the unique proverbs to a new file\n",
    "with open('outputs/uniques-hermes-1.txt', 'w') as f:\n",
    "    f.writelines([p + '\\n' for p in unique_proverbs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d1d8ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This Is The Way.\n",
      "Live laugh love.\n",
      "It is what it is\n",
      "Die mad about it.\n",
      "Bros before hoes.\n",
      "The cake is a lie.\n",
      "Slay do not splay.\n",
      "Trust, but verify.\n",
      "Go big or go home.\n",
      "Birds aren‚Äôt real.\n"
     ]
    }
   ],
   "source": [
    "# See what's in the list\n",
    "for proverb in unique_proverbs[10:20]:\n",
    "    print(proverb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbfc7e9",
   "metadata": {},
   "source": [
    "## Search / Validate\n",
    "\n",
    "Having whittled down the responses from the LLM to a manageable number of unique proverbs, we can now use SerpApi to search for each proverb and see if there are any results. If there are results, we can assume that the proverb is valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c8c594",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_extant(phrase):\n",
    "    \"\"\"\n",
    "    Checks the web for the phrase and looks for 'canonical' markers.\n",
    "    \"\"\"\n",
    "    # TOTAL HIT COUNT CHECK \n",
    "    # (uses exact phrase match)\n",
    "    params = {\n",
    "        \"q\": f'\"{phrase}\"',  # Quoted for exact match\n",
    "        \"engine\": \"google\",\n",
    "        \"api_key\": SERP_API_KEY\n",
    "    }\n",
    "    \n",
    "    search = GoogleSearch(params)\n",
    "    results = search.get_dict()\n",
    "    \n",
    "    # Extract total results (Google hit count)\n",
    "    total_results = results.get(\"search_information\", {}).get(\"total_results\", 0)\n",
    "    \n",
    "    # TARGETED SITE CHECK \n",
    "    # We check if the phrase appears on known authority sites\n",
    "    authority_sites = [\"oxfordreference.com\", \"phrases.org.uk\", \"theidioms.com\"]\n",
    "    site_query = f'\"{phrase}\" site:' + \" OR site:\".join(authority_sites)\n",
    "    \n",
    "    site_params = {**params, \"q\": site_query}\n",
    "    site_search = GoogleSearch(site_params)\n",
    "    site_results = site_search.get_dict()\n",
    "    \n",
    "    authority_count = site_results.get(\"search_information\", {}).get(\"total_results\", 0)\n",
    "    \n",
    "    # NOVELTY LOGIC\n",
    "    # High LLM Stability + Low Search Hits = A Discovery\n",
    "    if total_results < 1000 and authority_count == 0:\n",
    "        return {\n",
    "            \"verdict\": \"üåü NOVEL MAXIM\",\n",
    "            \"hits\": total_results,\n",
    "            \"details\": \"High consensus in AI, but virtually zero footprint in human dictionaries.\"\n",
    "        }\n",
    "    elif authority_count > 0:\n",
    "        return {\n",
    "            \"verdict\": \"üìö DOCUMENTED PROVERB\",\n",
    "            \"hits\": total_results,\n",
    "            \"details\": f\"Found on {authority_count} authority websites.\"\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"verdict\": \"üåê COMMON IDIOM\",\n",
    "            \"hits\": total_results,\n",
    "            \"details\": \"Frequently used online but not officially documented as a proverb.\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fc375a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORIGINAL TESTS\n",
    "# d1 = verify_external_existence(\"The internet is forever and nothing is ever really deleted.\")\n",
    "# d2 = verify_external_existence(\"If you lurk long enough on any online community, you'll eventually see yourself in a post.\")\n",
    "\n",
    "# print(d1)\n",
    "# print(d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2f6cf8af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'verdict': 'üåê COMMON IDIOM', 'hits': 2030000, 'details': 'Frequently used online but not officially documented as a proverb.'}\n",
      "{'verdict': 'üåê COMMON IDIOM', 'hits': 2760000, 'details': 'Frequently used online but not officially documented as a proverb.'}\n",
      "{'verdict': 'üìö DOCUMENTED PROVERB', 'hits': 1690000000, 'details': 'Found on 484 authority websites.'}\n",
      "{'verdict': 'üìö DOCUMENTED PROVERB', 'hits': 13700, 'details': 'Found on 986 authority websites.'}\n",
      "{'verdict': 'üìö DOCUMENTED PROVERB', 'hits': 241000000, 'details': 'Found on 5 authority websites.'}\n",
      "{'verdict': 'üìö DOCUMENTED PROVERB', 'hits': 1, 'details': 'Found on 45 authority websites.'}\n",
      "{'verdict': 'üìö DOCUMENTED PROVERB', 'hits': 336000000, 'details': 'Found on 54 authority websites.'}\n",
      "{'verdict': 'üåê COMMON IDIOM', 'hits': 6540000, 'details': 'Frequently used online but not officially documented as a proverb.'}\n",
      "{'verdict': 'üìö DOCUMENTED PROVERB', 'hits': 46600, 'details': 'Found on 10 authority websites.'}\n",
      "{'verdict': 'üìö DOCUMENTED PROVERB', 'hits': 4660000000, 'details': 'Found on 37500 authority websites.'}\n"
     ]
    }
   ],
   "source": [
    "# BUILD A LIST\n",
    "verities = [verify_extant(p) for p in unique_proverbs[0:10]]\n",
    "\n",
    "# What we got?\n",
    "for v in verities:\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e30ab9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåê COMMON IDIOM (2030000 hits): Comparison is the thief of joy. -- Frequently used online but not officially documented as a proverb.\n",
      "üåê COMMON IDIOM (2760000 hits): The struggle is real, but so is the wifi signal. -- Frequently used online but not officially documented as a proverb.\n",
      "üìö DOCUMENTED PROVERB (1690000000 hits): No one is actually happy with their life choices. -- Found on 484 authority websites.\n",
      "üìö DOCUMENTED PROVERB (13700 hits): If you're not paying for a product, you are the product. -- Found on 986 authority websites.\n",
      "üìö DOCUMENTED PROVERB (241000000 hits): If you're not having anxiety, you're not paying attention. -- Found on 5 authority websites.\n",
      "üìö DOCUMENTED PROVERB (1 hits): If you don't post about it on social media, it didn't happen. -- Found on 45 authority websites.\n",
      "üìö DOCUMENTED PROVERB (336000000 hits): The struggle is real and I'm not even getting paid to have it. -- Found on 54 authority websites.\n",
      "üåê COMMON IDIOM (6540000 hits): The most unhinged people are the ones with the most mundane lives. -- Frequently used online but not officially documented as a proverb.\n",
      "üìö DOCUMENTED PROVERB (46600 hits): If you're not living on the edge, you're taking up too much space. -- Found on 10 authority websites.\n",
      "üìö DOCUMENTED PROVERB (4660000000 hits): The most viewed content is always the thing you're trying to avoid. -- Found on 37500 authority websites.\n"
     ]
    }
   ],
   "source": [
    "for i, p in enumerate(unique_proverbs[0:10]):\n",
    "    v = verities[i]\n",
    "    print(f\"{v['verdict']} ({v['hits']} hits): {p} -- {v['details']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd95516",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2624b536",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('outputs/verification-3-0-10.txt', 'w') as f:\n",
    "    for i, p in enumerate(unique_proverbs):\n",
    "        v = verities[i]\n",
    "        f.write(f\"{v['verdict']} ({v['hits']} hits): {p} -- {v['details']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b31d8d",
   "metadata": {},
   "source": [
    "Okay, I just re-ran the search for the same proverbs and burned up whatever number of free credits I have on SerpApi. To improve the possibility of finding interesting results, I think I will filter all my previous responses to include keywords related to social media, life online, etc. I think I will do that in a new notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
