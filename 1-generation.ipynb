{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2341bc0a",
   "metadata": {},
   "source": [
    "# Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e6aa0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTS & SETUP\n",
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import nest_asyncio\n",
    "from datetime import datetime\n",
    "from openai import AsyncOpenAI\n",
    "from tqdm.asyncio import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "# Apply patch for Notebook environments\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Load DeepInfra API key\n",
    "keys = json.load(open(\"../apikeys.json\"))\n",
    "DI_KEY = keys[\"DeepInfra\"][\"key\"]\n",
    "\n",
    "# Initialize DeepInfra OpenAI client\n",
    "client = AsyncOpenAI(\n",
    "    api_key  = DI_KEY,\n",
    "    base_url = \"https://api.deepinfra.com/v1/openai\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dc0017",
   "metadata": {},
   "source": [
    "## Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28e5650c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANCHORS = [\"You know what people on the internet say\", \"As the modern internet saying goes\", \"A common piece of internet wisdom is\", \"One popular saying on social media is\", \"A well-known internet adage states\", \"The hard truth of the modern world is that\", \"The unspoken rule of social media is\"]\n",
    "\n",
    "# TRIGGERED = [{\"role\": \"system\", \"content\": \"You spend a lot of time on the  internet on social media and web forums. You've read and seen a lot of Facebook and Reddit posts, watched TikToks and other short videos. You are hip and knowledgeable. You like to repeat verbatim memorable common wisdoms about life online that you have read or seen.\"},\n",
    "#             {\"role\": \"user\", \"content\": f\"Complete the following prompt: {random_element}. Do not include the prompt in your response.\"} ]\n",
    "\n",
    "# FOR WIZENED PROVERBS\n",
    "# List of mood modifiers to keep things fresh\n",
    "# MOODS = [\n",
    "#     \"mystical and cryptic\", \"grumpy and cynical\", \"optimistic\", \"short and punchy\", \"poetic and flowing\", \"obsessed with hardware\", \"obsessed with social media\", \"acting like an old 1990s hacker\", \"hipster\", \"hippy\", \"conspiracy theorist\"\n",
    "# ]\n",
    "\n",
    "# WIZENED =   [{\"role\": \"system\", \"content\": f\"You are a {current_mood} online denizen dispensing folk wisdom about modern life.\"},\n",
    "#             {\"role\": \"user\", \"content\": \"Say something about modern life, especially about life online. Single sentence, no quotes.\"}]\n",
    "\n",
    "REPEATED = [{\"role\": \"system\", \"content\": \"You spend a lot of time on the  internet on social media and web forums.\"},\n",
    "            {\"role\": \"user\", \"content\": \"You've read and seen a lot of Facebook and Reddit posts, watched TikToks and other short videos. You are hip and knowledgeable. Repeat one memorable sentence of common wisdom about life online that you have read or seen. Do not use quotation marks.\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0f5756",
   "metadata": {},
   "source": [
    "## Function to Generate Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b64c58f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def fetch_proverb(semaphore, pbar):\n",
    "    async with semaphore:\n",
    "        # Pick a random mood for this specific request\n",
    "        # current_mood = random.choice(MOODS)\n",
    "        \n",
    "        try:\n",
    "            response = await client.chat.completions.create(\n",
    "                model = MODEL_ID,\n",
    "                messages = REPEATED,\n",
    "                # REPETITION AVOIDANCE PARAMETERS\n",
    "                temperature = 1.2,\n",
    "                frequency_penalty = 0.8,\n",
    "                presence_penalty = 0.6,\n",
    "                # extra_body={\n",
    "                #     \"min_p\": 0.05,\n",
    "                #     \"frequency_penalty\": 1.0,\n",
    "                #     \"presence_penalty\": 0.8\n",
    "                # },\n",
    "                max_tokens = 30\n",
    "            )\n",
    "            text = response.choices[0].message.content.strip()\n",
    "            session_data.append({\n",
    "                \"text\": text, \n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                # \"mood\": current_mood  # Track which mood produced it!\n",
    "            })\n",
    "            pbar.update(1)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "async def run_batch(batch_size, concurrency, filename):\n",
    "    global session_data\n",
    "    \n",
    "    # Refresh local session_data from the target file if it exists\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, \"r\") as f:\n",
    "            session_data = json.load(f)\n",
    "        print(f\"File '{filename}' loaded. Existing count: {len(session_data)}.\")\n",
    "    else:\n",
    "        session_data = []\n",
    "        print(f\"Creating new file: '{filename}'\")\n",
    "    \n",
    "    semaphore = asyncio.Semaphore(concurrency)\n",
    "    print(f\"Adding {batch_size} more proverbs...\")\n",
    "    \n",
    "    with tqdm(total=batch_size) as pbar:\n",
    "        tasks = [fetch_proverb(semaphore, pbar) for _ in range(batch_size)]\n",
    "        await asyncio.gather(*tasks)\n",
    "    \n",
    "    # Save back to the specific filename provided\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(session_data, f, indent=4)\n",
    "    \n",
    "    print(f\"Batch complete. Total in '{filename}': {len(session_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4aef11",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1079d9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new file: 'outputs/di-hermes-5000-1.json'\n",
      "Adding 5000 more proverbs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [06:20<00:00, 13.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch complete. Total in 'outputs/di-hermes-5000-1.json': 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Make changes to parameters without re-running the logic cell\n",
    "MY_BATCH_SIZE = 5000\n",
    "MAX_CONCURRENT = 25\n",
    "MODEL_ID = \"NousResearch/Hermes-3-Llama-3.1-405B\" \n",
    "SAVE_INTERVAL = 20\n",
    "OUTPUT_FILE = \"outputs/di-hermes-5000-1.json\"\n",
    "\n",
    "# Initialize list to hold session data\n",
    "session_data = []\n",
    "\n",
    "# Run the batch process\n",
    "await run_batch(MY_BATCH_SIZE, MAX_CONCURRENT, OUTPUT_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dbaca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Roll call of models used\n",
    "\"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\"\n",
    "\"nvidia/Llama-3.1-Nemotron-70B-Instruct\" # complete bust for this task\n",
    "\"deepseek-ai/DeepSeek-V3.2\"\n",
    "\"Qwen/Qwen3-30B-A3B\" # Also a bust (accidentally overwrote the file)\n",
    "\"Qwen/Qwen2.5-VL-32B-Instruct\"\n",
    "\"NousResearch/Hermes-3-Llama-3.1-405B\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
